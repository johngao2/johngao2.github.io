---
layout: post
title: "Optimal Real-time Ad Bidding with Reinforcement Learning"
author: "John Gao"
categories: blog
tags: [RL, RLB]
---

In this **pretty technical** post I'll be going over a recent paper which applies Reinforcement Learning **in industry** to train an agent which bids on web ads in real time. I'll cover the process of defining the real-time bidding problem, formulating this problem into an MDP, solving that MDP with dynamic programming, and scaling up the algorithm with the help of deep neural nets. I'll also be discussing experiment flow & results of this particular method.

* The generated Toc will be an ordered list
{:toc}

# Introductions & Topic Motivation

>"The excitement and PR hype behind reinforcement learning is a bit disproportionate relative to the economic value it's creating today" - Andrew Ng

Reinforcement learning has been a research interest of mine for a while. However, one common gripe that I and many others have is the fact that RL doesn't seem to work well in the real world. Sure, it's great for games ([AlphaGo](https://deepmind.com/research/alphago/){:target="_ blank"}, [OpenAI Five](https://openai.com/five/){:target="_ blank"}), but what good is a supposedly "general" framework like RL if it can't be applied to real-life problems that actually need solving? I was also puzzled by the fact that RL, despite being based on (and sometimes literally the same thing as) widely applied late 20th control theory, is not nearly as widely applied itself.

As a result, I started researching examples of reinforcement learning being applied in industry. The paper I'm covering in this post ([Cai et al., 2017](https://arxiv.org/abs/1701.02490){:target="_ blank"}) is one of these rare examples. This paper isn't perfect: the end result isn't as impressive as I thought it would be, the notation tended to get verbose at times. However, despite all that, I still think that this is an amazing paper solely because it shows that RL **can** create economic value. That's the main reason why I'm covering this paper.

# Overview of Real-Time Ad Bidding

Each time a user sees an ad on a site, we call that an "impression". One webpage can have multiple panes for ads, so multiple impressions can be generated with a single page visit. There's multiple ways that a website can charge companies for impressions. One such way is to auction out each impression in real time, in a process called **Real-Time Bidding(RTB)**. Here's what that process looks like:  

The process starts off when a user loads a webpage. While the page is loading, the following events happen in sequence:
1. The website sends impression info (e.g. user cookie ID, location, time, domain, URL, etc) to a central auction exchange as a **bid request**
2. Bots from various companies will receive this bid request, and look at the info.
3. Each bot will decide on a bid price, and submit those prices. Each bot will also submit the ad itself that it wants to broadcast, should it win the auction.
4. The highest bidder will have their ad displayed on the webpage.

Keep in mind that this process happens in the span of milliseconds, and is repeated every time someone visits a page. To make this process clearer, here's a gif outlining the process for a hypothetical ad.

![Alt Text](../assets/img/rlb/rtb_process.gif){:height="700px" width="2000px"}
**Figure 1: RTB Process GIF**
{: style="text-align: center"}

The goal here is to design an agent that makes bids which satisfy budget constraints while maximizing some KPI. There's various options for this, but we'll be using the most common one, which is **clickthrough-rate(CTR)**.

# Background and Prior Work

When a user clicks on an impression, they might buy things from the company being advertised, or they might not. The average spend from users who click the ad is referred to as **click value**, and is similar to conversion rate. The probability of a user actually clicking the ad after seeing it is called **click-through rate(CTR)**. In an ideal world, the cost of an impression is the value of the click multiplied by the click-through rate. This makes sense intuitively; it's simply the expected value of the revenue from showing someone a particular ad. Finding the optimal bid price in this scenario is relatively straightforward; just predict click value and CTR, then submit their product as a bid.

Unfortunately, we live in the real world, where things aren't so simple. The optimal price of an ad depends many other factors, such as **market competition**, **remaining auction volume**, and **remaining campaign budget**. To bid properly, will agents need to model all of these variables to some extent. In the past, people have tried to simply fit distributions to these variables, and then maximize some metric (e.g. total clicks or revenue) based on these distributions. However, this tended not to work so well, since market competition tends to be very dynamic, making it very hard to have accurate static models.

One main difference between this paper and previous papers is that the authors here treat bidding as a sequence of events over time, rather than independent events. This sequential view of the bidding problem allows the non-stationary nature of market competition to be circumvented.

## Previous Work on Bidding Agents

After receiving a bid request, an agent generally needs to do three things:
1. Estimate the **utility** of the impression (represented in this paper by CTR, but can use other metrics). This part is called the **utility estimation** component. We need to know how much the impression would benefit the company, and this is probably the most important decider of bid price. For example, if an agent thinks that a certain user is very unlikely to bid on an impression, then it'll bid low.
2. Forecast the **cost distribution** of the ad. This is the **bid landscape forecasting** component of the agent. A cost distribution is needed in order to find the win probability for a given bid on an impression. One might ask: "why take the cost distribution into account at all? Why not just bid the true utility of the ad?". The response to this is that in situations where the market price for an ad is much lower than the utility to our company, bidding the true utility would result in highballing the competition and wasting money.
3. Given estimated utility and cost distribution, and using known remaining budget & auction volume, decide on a final bid price. This is referred to as the **bid optimization** component. Work on this particular component is pretty scarce, which is good since **this component is the meat and potatoes of this paper.**

### Utility Estimation Component

* probability estimation task (e.g. will user click, will user convert)
* many ways to measure, e.g. CTR, conversion, post-click conversion patterns
* many ways to model incl any supervised learning algo
* can also model online with immediate updating e.g. Bayesian probit, FTRL logistic regression

## estimating cost

* need to model price distribution for specific ad, whose CDF is win probability given a bid price
* data is censored, you can only observe price if you win
* one method is to estimate log-normal price distribution using GBT's to estimate parameters, treat it like normal supervised learning and train only on uncensored data
* another method is to use censored lin reg to model winning prices and censored losing prices at the same time
* 3rd method is to use survival analysis + decision trees where each leaf is a nonparametric survival model
* this paper uses a Kaplan-Meier estimator that uses both winning and losing data to predict market price

## optimizing bid

* most common is linear-bidding function w.r.t. predicted utility, but can use nonlinear functions as well
* these methods fall short when the data isn't stationary (it's not IRL)
* good thing that previous literature on this is lacking because it's the **meat and potatoes** of this paper

# The Problem

## Problem definition (possibly skip this and go straight to MDP)

* Although real time bidding was described above, will describe more formally here
* Bidding is an episodic process, with each episode comprising T auctions sent to the agent in sequence
* Each auction represented by high-d feature vector, containing ad info (ad creative ID, campaign ID), and specific impression info (user cookie ID, location, time, domain, URL)
* Agent set to max clicks given T auctions, a budget, and feature vectors for each ad
* IF agent bids above market price of the ad, it wins and remaining budget is decreased. Agent observes user response and market price.
* IF agent loses bid, then it loses predicted CTR, but budget stays the same
* After T auctions are reached, both budget and volume are reset and a new campaign starts.

## MDP formulation

* copy in notation table
* tell ppl to refer to tutorial for MDP basics
* I'll stick to the same notation that they use here, which may be different from what you've seen but the ideas are the same
* **state:** $$s$$ represented by the tuple $$s=(t,b, \boldsymbol{x_t})$$
  - total state space: $$\boldsymbol{S} = \{0,...,T\} \times \{0,...,B\} \times \boldsymbol{X}$$
  - $$t$$ is really just a 'time step'. I prefer to call this a "stage", but you can also just consider it as another stage, as the authors have.
    - Still, I'm kinda mad that the authors didn't just do $$s_t$$ or something.
  - $$b$$ is the remaining budget
  - $$\boldsymbol{x_t}$$ is the feature vector for an ad
  - As you can see, the state space can get pretty ridiculous especially considering we have to factor in all possible future vectors $$\boldsymbol{x_t}$$
  - However, you'll see in the future we can ignore predicting x_t if we assume they're iid
  - t counts down with each bid, so when $$t=0$$ the agent resets and a new campaign starts
* **action:** Set of all actions in a state is just how much you can bid on it, which depends only on remaining budget
  - action space for any state: $$\boldsymbol{A_s} = \boldsymbol{A_{(t, b, \boldsymbol{x_t})}} = {0,...,B}$$
  - realistically, bids need to be in increments in 1 cent, so the action state is discrete
  - also, we'll set an upper limit on possible actions to prevent agent bidding something ridiculous like $20 on an ad (also saves computing time)
* **state transition:** Remember that the state is represented by 3 variables $$(t, b, \boldsymbol{x_t})$$, so let's see how those change over time
  - $$t$$: this represents # of remaining ads, so it decreases by 1 each time the agent observes a state. So in every case $$t \rightarrow t-1$$
  - $$b$$: the remaining budget will decrease by $$a$$ *if* $$a$$ is the highest price, which would result in a winning bid. The probability of winning is $$\sum\nolimits_{\delta =0}^{a}m(\delta,\boldsymbol{x})$$, where $$\delta$$ is some market price. This probability is just the area of the market p.d.f. from 0 to $$a$$. Likewise, the probability of not winning is $$1-\sum\nolimits_{\delta =0}^{a}m(\delta,\boldsymbol{x})=\sum\nolimits_{\delta =a+1}^{\inf}m(\delta,\boldsymbol{x})$$ Therefore, the full state transition for $$b$$ is:
  $$
  b \rightarrow
  \begin{cases}
  b-a & \text{with prob. $\sum\nolimits_{\delta =a+1}^{\inf}m(\delta,\boldsymbol{x})$ (win bid)} \\
    b & \text{with prob. $\sum\nolimits_{\delta =0}^{a}m(\delta,\boldsymbol{x})$ (lose bid)} \\
  \end{cases}
  $$

# References
[1] Han Cai, Kan Ren, Weinan Zhang, Kleanthis Malialis, Jun Wang, Yong Yu, and Defeng Guo. 2017. Real-time bidding by reinforcement learning in display advertising. In Proceedings of the Tenth ACM International Conference on Web Search and Data Mining. ACM, 661â€“670.
