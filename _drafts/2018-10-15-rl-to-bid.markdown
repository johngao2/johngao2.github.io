---
layout: post
title: "Optimal Real-time Ad Bidding with Reinforcement Learning"
author: "John Gao"
categories: blog
tags: [RL, RTB]
---

In this **technical and long** post I'll be summarizing a recent paper which applies reinforcement learning **in industry** to train an agent which bids on web display ads in real time. I'll cover the process of formulating the real-time problem into an MDP, solving that MDP with dynamic programming, and scaling up the algorithm for bigger datasets with the help of deep neural nets. I'll also be discussing the experiments & results of the paper.

* The generated Toc will be an ordered list
{:toc}

# Introduction & Topic Motivation

>"The excitement and PR hype behind reinforcement learning is a bit disproportionate relative to the economic value it's creating today" - Andrew Ng

Reinforcement learning has become a research interest of mine recently. However, one common gripe that I and many others have is the fact that RL doesn't seem to work well in the real world. Sure, it's great for games ([AlphaGo](https://deepmind.com/research/alphago/){:target="_ blank"}, [OpenAI Five](https://openai.com/five/){:target="_ blank"}) and sometimes robotics, but what good is a supposedly "general" framework like RL if it can't be applied to real-life problems that actually need solving? I was further puzzled by the fact that RL, despite being based on (and sometimes literally the same thing as) widely applied late 20th control theory, is not nearly as widely applied itself.

As a result, I started researching examples of reinforcement learning being applied in industry. The paper I'm covering in this post ([Cai et al., 2017](https://arxiv.org/abs/1701.02490){:target="_ blank"}) is one of these rare examples. This paper isn't perfect: the end result tended to be a bit suspicious, the proofs were verbose at times. However, despite all that, I still think that this is an amazing paper solely because it shows that RL **can** create economic value.

# The Real-Time Bidding Process

If you're one of the 5 people in the world who doesn't use Adblock, you'll tend to see visual ads that look like banners or images. We call these **display advertisements**. Each time a user sees an ad on a site (doesn't need to be unique), we call that an **impression**. For the purposes of this post, we'll use the terms ad and impression interchangeably. Multiple impressions can be generated with a single page visit, since webpages tend to have more than one ad slot.

There are multiple ways that a website can charge companies for impressions. One such way is to auction out each impression in real time, in a process called **Real-Time Bidding(RTB)**. Here's what that process looks like:  

The process starts off when a user reaches a webpage. While the page is loading, the following events happen in sequence:
1. The website sends impression info (e.g. user cookie ID, location, time, domain, URL, etc) to a central auction exchange as a **bid request**
2. Bots from various companies will receive this bid request, and look at the info.
3. Each bot will decide on a bid price, and submit those prices. Each bot will also submit the ad itself that it wants to broadcast, should it win the auction.
4. The highest bidder will have their ad displayed on the webpage.

Keep in mind that this process happens in the span of milliseconds, and is repeated every time someone visits a page. To make this process clearer, here's a gif outlining the process for a hypothetical ad.

![Alt Text](../assets/img/rlb/rtb_process.gif){:height="700px" width="2000px"}
**Figure 1: RTB Process GIF**
{: style="text-align: center"}

The goal here is to design an agent that makes bids which satisfy budget constraints while maximizing some KPI. There's various options for this, but we'll be using the most common one, which is **click-through rate(CTR)**.

# Background and Prior Work

When a user clicks on an impression, they might buy things from the company being advertised, or they might not. The average spend from users who click the ad is referred to as **click value**, and is similar to conversion rate. The probability of a user actually clicking the ad after seeing it is called **click-through rate(CTR)**. In an ideal world, the cost of an impression is the value of the click multiplied by the click-through rate. This makes sense intuitively; it's simply the expected value of the revenue from showing someone a particular ad. Finding the optimal bid price in this scenario is relatively straightforward; just predict click value and CTR, then submit their product as a bid.

Unfortunately, we live in the real world, where things aren't so simple. The optimal price of an ad depends many other factors, such as **market competition**, **remaining auction volume**, and **remaining campaign budget**. To bid properly, will agents need to model all of these variables to some extent. In the past, people have tried to simply fit distributions to these variables, and then maximize some metric (e.g. total clicks or revenue) based on these distributions. However, this tended not to work so well, since market competition tends to be very dynamic, making it very hard to have accurate static models.

One main difference between this paper and previous papers is that the authors here treat bidding as a sequence of events over time, rather than independent events. This sequential view of the bidding problem allows the non-stationary nature of market competition to be circumvented.

## Past Bidding Agents

After receiving a bid request, an agent needs to do three things:
1. Estimate the *utility* of the impression (represented in this paper by CTR, but can use other metrics). This part is called the **utility estimation component**. This is basically predicting how much benefit the company will receive from showing the ad, and is the most important decider of bid price. For example, if an agent thinks that a certain user is very unlikely to click on an impression, then it'll bid low.
2. Forecast the *cost distribution* of the ad. This is the **bid landscape forecasting component** of the agent. A cost distribution is needed in order to find the win probability for a given bid on an impression. One might ask: "why take the cost distribution into account at all? Why not just bid the true utility of the ad?". The response to this is that in situations where the market price for an ad is much lower than the utility to our company, bidding the true utility would result in highballing the competition and wasting money.
3. Given estimated utility and cost distribution, and using known remaining budget & auction volume, decide on a final bid price. This is referred to as the **bid optimization component**. Past work on this particular component is pretty scarce, which is good since **this component is the main focus of this paper.**

**Utility Estimation Component:**  
The goal here is to estimate the probability of of an event (e.g. user click) given ad info and user info. These probabilities are pretty stationary, which makes this component a fairly typical supervised learning problem. Therefore, any old supervised learning model will do here.

In practice, companies tend to use [logistic regression](http://wnzhang.net/share/rtb-papers/cvr-est.pdf){:target="_ blank"} or gradient boosted trees for these types of tasks. There are also cases of more niche models that perform weight updates automatically, such as [Bayesian probit regression](http://quinonero.net/Publications/AdPredictorICML2010-final.pdf){:target="_ blank"} or ["Follow the Regularized Leader" (FTRL) logistic regression](http://www.eecs.tufts.edu/~dsculley/papers/ad-click-prediction.pdf){:target="_ blank"}. Since utility estimation isn't the focus, the paper covered in this post ([Cai et al., 2017](https://arxiv.org/abs/1701.02490){:target="_ blank"}) simply uses logistic regression to predict CTR, with ad info serving as features.

**Cost Estimation Component (a.k.a. Bid Landscape Forecasting):**  
The bidding agent will also need to model the price distribution for a particular ad. This distribution is needed to find win probability, which is just the c.d.f. of the distribution at the bid price. Unfortunately, this isn't a straightforward supervised learning problem like utility estimation.

The main problem here is that the price is **censored** for most ads. Specifically, it's **right-censored**, meaning that we will often not know how high a bid price can be. This is because the agent can only observe a market price for an impression when it wins that auction. Of course, any one agent is likely to lose the vast majority of its auctions, so dealing with censored auction prices is important.

Of course, one could just ignore the censored data and just use whatever data is available to fit a distribution. For example, [this paper](http://wnzhang.net/share/rtb-papers/bid-lands.pdf){:target="_ blank"} uses gradient boosted trees to fit a log-normal price distribution. The problem with this approach is that actual uncensored data can be scarce, depending on the budget given to the agent. Furthermore, markets conditions are quite non-stationary, which makes it even more vital to use all the data that's available.

A possible solution to this problem is to use [censored linear regression](https://dl.acm.org/citation.cfm?id=2783276){:target="_ blank"} to model likelihoods for both censored and observed prices. Bid landscape forecasting isn't the focus in ([Cai et al., 2017](https://arxiv.org/abs/1701.02490){:target="_ blank"}), so the authors follow the method of a previous [paper](http://wnzhang.net/papers/unbias-kdd.pdf){:target="_ blank"}, which uses a Kaplan-Meier estimator to predict the market price distribution. This particular estimator comes from survival analysis, and is well suited to right-censored data.

**Bid Optimization Component:**  
In industry, the most widely used method for bid price optimization is to use a [linear bidding function](https://dl.acm.org/citation.cfm?id=2339655){:target="_ blank"}, using predicted CTR as the input. It basically works like this:

$$
\text{bid}=b_0*\frac{\text {pred. current ad utility}}{\text {avg. historical utility over all ads}}
$$

Where $$b_0$$ is some multiplier that's fitted using past data. Pretty simple, right? It actually works surprisingly well. Other researchers have also proposed [non-linear bidding functions](https://arxiv.org/abs/1506.03837){:target="_ blank"} similar to the one above. However, both of these methods don't play well with real world non-stationary data, since they're static in nature.

This brings us to the end of literature review. The rest of this post will cover new stuff from ([Cai et al., 2017](https://arxiv.org/abs/1701.02490){:target="_ blank"})

# Formulating the RTB problem as a Markov Decision Process

The Real Time Bidding (RTB) process was described earlier, but I'll cover more specifically the details of what the agent needs to do.

Consider an advertising campaign, where a company wants to allocate a certain budget over the next $$t$$ impressions. The agent's job would then be to maximize total clicks (can use other metrics, but we'll stick to # of clicks) given this limited budget and limited number of impressions. It accomplishes this job by being fed a bunch of impressions in sequence, and deciding how much to bid on each one. It makes this decision considering three pieces of information:
1. The number of ads remaining in the campaign
2. The amount of remaining budget in the campaign
3. The specific info for the ad (cookie ID, URL, etc)

That's the problem in a nutshell.

In order to tackle the RTB problem with a reinforcement learning algo, we'll need to formulate this problem as a Markov Decision Process(MDP's). If you're new to MDP's, one place to learn about them is my [tutorial](http://www.johngao.com/blog/rl-intro-beginners-1.html){:target="_ blank"}.

There are 4 main parts to our MDP formulation. These are:
1. State
2. Action
3. State Transition
4. Reward

Some of you may be wondering why there's no discount factor, which is normally included in MDP formulations. The reason for this is that we set the discount factor $$\gamma$$ to a constant value of 1. This is because the time at which the agent receives clicks doesn't matter, since we're trying to maximize total clicks over the course of a campaign.

**State:**  
Before we dive into each part of the MDP, we'll cover the relevant notation first.

|      Notation       | Description                                                                                                                                 |
|:-------------------:|:--------------------------------------------------------------------------------------------------------------------------------------------|
|        $$t$$        | A counter representing the number of impressions remaining in the campaign. Essentially a "time step". Also used to identify an impression. |
|        $$T$$        | Total number of impressions in the campaign.                                                                                                |
|        $$b$$        | The amount of remaining budget.                                                                                                             |
|        $$B$$        | Total budget of a campaign at the beginning.                                                                                                |
|     $$\pmb{x}$$     | A feature vector that contains info for a particular impression (e.g. cookie ID, URL, time of day, etc.)                                    |
|    $$\pmb{x}_t$$    | The feature vector for a particular ad $$t$$                                                                                                |
|        $$s$$        | The state variable. $$s$$ is actually a 3-tuple of other variables.                                                                         |
|     $$\pmb{S}$$     | Set of all possible states                                                                                                                  |
| $$(t, b, \pmb{x})$$ | 3-tuple which is equivalent to $$s$$.                                                                                                       |

**Table 1: MDP State Notation**
{: style="text-align: center"}

As you can see from the notation table, our state $$s \in \pmb S$$ is made up of three state variables: $$t$$, $$b$$, and $$\pmb x$$. These are the three pieces of info that the agent needs to process in order to decide on a bid price for each impression.

The 'time step' $$t$$ tracks how many impressions are left at a campaign, and is also also known as the remaining ad volume. This is important because we want the agent to know how many impressions it has left, so that it can decide to spend more liberally if there are few ads and a lot of budget, or vice versa. Many researchers wouldn't actually consider this a state, but instead treat it as a separate time step variable, so that they can express stuff like $$s_t$$, which refers to a state at a particular point in the campaign. I personally like to refer to $$t$$ as the 'stage'. However, treating $$t$$ as another state isn't wrong, so we'll stick with this interpretation for consistency's sake.

The budget variable $$b$$ simply tracks how much money the agent has left to spend in a campaign. The agent will need to keep this in mind when making decisions about how much to spend. One thing to note is that since the lowest monetary denominator is 1 cent, the budget variable is discrete. In this post, cents will be the unit of choice for money, so $$b$$ will be expressed in cents.

The feature vector $$\pmb {x}_t$$ represents all of the impression info in vector form, for a particular impression $$t$$. **The only purpose of this feature vector is to be used as inputs for the expected click-rate model**.

**Action:**  

|            Notation             | Description                                        |
|:-------------------------------:|:---------------------------------------------------|
|    $$a_{(t, b, \pmb{x}_t)}$$    | The bid price chosen in a particular state         |
|             $$a_t$$             | Shorthand for $$a_{(t, b, \pmb{x}_t)}$$            |
|             $$a_s$$             | Shorthand for $$a_{(t, b, \pmb{x}_t)}$$            |
| $$\pmb{a}_{(t, b, \pmb{x}_t)}$$ | The set of available actions in a particular state |

**Table 2: MDP Action Notation**
{: style="text-align: center"}

The action is pretty straightforward to define; it's just how much the agent decides to bid for a particular impression. Like $$b$$, $$a_t$$ represents a monetary amount, and is therefore discrete with intervals of 1 cent between each value. The lower bound for and $$a_t$$ is 0, but the higher bound is hypothetically whatever the agent has left to spend, which is $$b$$. However, we don't want the agent to spend ridiculous amounts like $20 for one impression, so we usually cap the amount that the agent can spend on any ad.

**State Transitions:**

|        Notation        | Description                                                                                                                     |
|:----------------------:|:--------------------------------------------------------------------------------------------------------------------------------|
|   $$\mu (s, a, s')$$   | The probability of a state transitioning from some state $$s$$ to some state $$s'$$ given action $$a$$.                         |
|       $$\delta$$       | The market price, which is the highest market bid for an ad. Our agent wins an auction if $$a>\delta$$ for that ad.             |
|     $$m(\delta)$$      | The p.d.f. of the market price $$\delta$$. The c.d.f. of this at a certain bid $$a=\delta$$ is the win probability of that bid. |
| $$m(\delta, \pmb{x})$$ | The p.d.f. of the market price $$\delta$$ for a certain ad. This is a more accurate version of $$m(\delta)$$                    |

**Table 3: MDP State Transition Notation**
{: style="text-align: center"}

We have three state variables, so let's look at each of their transitions.

$$t$$ is the easy one, since the number of ads remaining in a campaign always goes down by 1 each time we bid on an ad. Therefore:  

$$
t \rightarrow t-1
$$

$$b$$ is a bit more complex. Basically, when the agent makes a bid, it puts a certain amount $$a$$ of its remaining budget on the line. If it wins the auction (i.e. if $$a>\delta$$) then the remaining budget decreases by $$a$$, since the ad needs to be paid for. If the agent doesn't win (i.e. $$a<\delta)$$, then the budget doesn't change. The probability of a win is the area of $$m(\delta)$$ where $$\delta<a$$, which is $$\sum\nolimits_{\delta =0}^{a}m(\delta)$$. Likewise, the probability of a loss is $$1-\sum\nolimits_{\delta =0}^{a}m(\delta)$$, which can also be expressed as $$\sum\nolimits_{\delta =a+1}^{\infty}m(\delta)$$ In other words, the state transition for $$b$$ is:

$$
b \rightarrow
\begin{cases}
b-a & \text{with prob. $\sum\nolimits_{\delta =a+1}^{\infty}m(\delta)$ (win bid)} \\
  b & \text{with prob. $\sum\nolimits_{\delta =0}^{a}m(\delta)$ (lose bid)} \\
\end{cases}
$$

The problem with this is that we don't know what the market price distribution is. Thankfully, this is the job of the **bid landscape forecasting** component. This component isn't the focus of this post, so for now, treat it as a black box that spits out an estimated $$m(\delta)$$ which we then use in our bidding agent. If we have the feature vector for the ad, then we can get a more accurate distribution $$m(\delta, \pmb{x}$$), which replaces $$m(\delta)$$. If you want to know more about bid landscape forecasting, check out ([Zhang et al. 2016](http://wnzhang.net/papers/unbias-kdd.pdf){:target="_ blank"}).

Long story short, here's the full state transition:

$$
(t, b) \rightarrow
\begin{cases}
(t-1, b-a) & \text{with prob. $\sum\nolimits_{\delta =a+1}^{\infty}m(\delta)$} \\
  (t-1, b) & \text{with prob. $\sum\nolimits_{\delta =0}^{a}m(\delta)$} \\
\end{cases}
$$

or, if we have the feature vector:

$$
(t, b) \rightarrow
\begin{cases}
(t-1, b-a) & \text{with prob. $\sum\nolimits_{\delta =a+1}^{\infty}m(\delta, \pmb{x})$} \\
  (t-1, b) & \text{with prob. $\sum\nolimits_{\delta =0}^{a}m(\delta, \pmb{x})$} \\
\end{cases}
$$

**Reward:**

|        Notation         | Description                                                                                                               |
|:-----------------------:|:--------------------------------------------------------------------------------------------------------------------------|
|      $$r(s,a,s')$$      | The reward when the agent transitions from some state $$s$$ to some state $$s'$$ given action $$a$$.                      |
|   $$\theta(\pmb{x})$$   | The predicted click probability/CTR of an ad with feature vector $$\pmb x$$                                               |
| $$\theta_{\text{avg}}$$ | The avg CTR over all impressions. Used as substitute for $$\theta(\pmb{x})$$ when we don't know feature vector $$\pmb x$$ |

**Table 4: MDP Reward Notation**
{: style="text-align: center"}

The difficulty in designing good reward functions is a major roadblock in the practicality of reinforcement learning. Fortunately for our case, it's pretty straightforward!

We know that we want the agent to maximize total clicks over a campaign. Therefore, our best bet is to give it some constant reward, like 1, each time one of its ads is clicked on. If an agent shows an ad that isn't clicked, or doesn't win the auction for an ad, then it gets a reward of 0.

The agent doesn't know the click probability, or CTR, of the impression that it's bidding on. However, it does have the ad's feature vector $$\pmb{x}$$. Using this feature vector, the agent can predict the CTR with the **utility estimation component**. The predicted CTR for the current impression is represented by $$\theta(\pmb{x})$$. Like the bid landscape forecasting component, please treat this part as a black box. Just know that the utility estimation component outputs out an estimated click rate $$\theta(\pmb{x})$$ for each impression using that impression's feature vector $$\pmb x$$ (more details [here](http://wnzhang.net/share/rtb-papers/cvr-est.pdf){:target="_ blank"}).

The agent will also need to predict rewards for future ads it hasn't seen yet, in order to judge the utility of saving money for later. Unfortunately, since we can't predict the future feature vectors, we also can't use the utility estimation component. Therefore, the agent just uses the historical average CTR $$\theta_{\text{avg}}$$ as a substitute for $$\theta (\pmb{x})$$ for future ads.

## MDP Recap

Just a quick recap of what we have to work with:  
**State**: $$(t, b)$$ - The remaining ad volume and bid price  
**Action**: $$a_{(t, b)}$$ - The amount to bid for a particular impression  
**State transition**:  
$$
(t, b) \rightarrow
\begin{cases}
(t-1, b-a) & \text{with prob. $\sum\nolimits_{\delta =a+1}^{\infty}m(\delta)$} \\
  (t-1, b) & \text{with prob. $\sum\nolimits_{\delta =0}^{a}m(\delta)$} \\
\end{cases}
$$  
or  
$$
(t, b) \rightarrow
\begin{cases}
(t-1, b-a) & \text{with prob. $\sum\nolimits_{\delta =a+1}^{\infty}m(\delta, \pmb{x})$} \\
  (t-1, b) & \text{with prob. $\sum\nolimits_{\delta =0}^{a}m(\delta, \pmb{x})$} \\
\end{cases}
$$  
**Reward**:  
$$\theta(\pmb{x})$$ - The predicted CTR of an impression with feature vector $$\pmb x$$  
or  
$$\theta_{\text{avg}}$$ - The predicted CTR of any impression

# Solving the RTB problem

In this section, I'll go over the actual RL algorithm that's used to find optimal bid prices. The larger the campaign volume, the more computing power is required to solve the MDP, so I'll first cover the solution for small campaigns with fewer than 1000 ads ($$T<1000$$). Afterwards, I'll cover the solution for bigger campaign sizes, which is built from the small scale solution.

|       Notation        | Description                                                                                                               |
|:---------------------:|:--------------------------------------------------------------------------------------------------------------------------|
|    $$V_{*}(t,b)$$     | The optimal value. This is the total future rewards from state $$(t,b) assuming all actions are optimal.                  |
| $$a_{*}(t,b,\pmb x)$$ | The optimal action in state $$(t,b)$$ with feature vector $$\pmb x$$                                                      |
|       $$g(a)$$        | Equivalent to $$\Big(\theta(\pmb x)+V_{* }(t-1,b-a)-V_{* }(t-1,b)\Big)$$. Most important part of optimal action function. |
|      $$D(t,b)$$       | Equivalent to $$V_{* }(t,b+1)-V_{* }(t,b)$$, used to find the value portion of $$g(a)$$ without finding values directly.  |
|      $$NN(t,b)$$      | The neural network approximation of $$D(t,b)$$.                                                                           |

**Table 5: MDP Solution Notation**
{: style="text-align: center"}

## Solving the Small Scale MDP (RLB)

The authors in (Cai et al. 2017) use a version of dynamic programming called value iteration to solve the MDP, and you can learn more about that specific method [here](https://medium.com/@m.alzantot/deep-reinforcement-learning-demysitifed-episode-2-policy-iteration-value-iteration-and-q-978f9e89ddaa){:target="_ blank"}. This solution for small MDP's is referred to by the authors as "Reinforcement Learning to Bid", or **RLB**.

The optimal value function, which represents total future rewards, is the following:  

$$
V_{* }(t,b) \approx \max\limits_{0 \leq a \leq b} \left\{ \sum\limits_{\delta =0}^{a}m(\delta)\Big(\theta_{\text{avg}}+V_{* }(t-1,b-a)\Big) + \sum\limits_{\delta =a+1}^{\infty}m(\delta)\Big(V_{* }(t-1,b)\Big) \right\}
$$

The first summation represents the consequences of winning the an auction, which is composed of the (predicted) reward for the impression plus the value for a future state where the budget is decreased by $$a$$. Likewise, the second summation describes the consequences for losing the current auction, which would yield no reward but also no decrease in budget. Also, this function is approximate since the full function is actually $$V(t,b,\pmb x)=...$$. However, (Cai et al. 2017) marginalizes out $$\pmb x$$ in a long series of derivations that I won't go over here. Intuitively this makes sense, since it's very difficult to predict future values of $$\pmb x$$, and in turn, $$\theta(\pmb x)$$. Therefore, replacing it with the historical average CTR $$\theta_{\text{avg}}$$ is fair, so this approximate function will be the one that's used in the solution of this MDP. Another consequence marginalizing out $$\pmb x$$ is that we have to use the less accurate market distribution $$m(\delta)$$ rather than $$m(\delta, \pmb{x})$$.

The optimal action for each impression is that which maximizes the value function:

$$
a_{* }(t,b,\pmb x) = \arg\max\limits_{0 \leq a \leq b} \left\{ \sum\limits_{\delta =0}^{a}m(\delta, \pmb{x})\Big(\theta(\pmb x)+V_{* }(t-1,b-a)\Big) + \sum\limits_{\delta =a+1}^{\infty}m(\delta, \pmb{x})\Big(V_{* }(t-1,b)\Big) \right\}
$$

As you can see, the optimal action formula is simply the argmax of the value function. One key change is that we do have the feature vector for the current ad, so we can use $$\theta(\pmb x)$$ instead of $$\theta_{\text{avg}}$$ in order to get a more accurate reward prediction. Likewise, we can use $$m(\delta, \pmb{x})$$ instead of $$m(\delta)$$.

To simplify the action function, we know that:  

$$
\sum\limits_{\delta=0}^{\infty}m(\delta, \pmb{x}) = \sum\limits_{\delta=0}^{a}m(\delta, \pmb{x}) + \sum\limits_{\delta=a+1}^{\infty}m(\delta, \pmb{x}) = 1
$$  

Which means that:  

$$
\sum\limits_{\delta=a+1}^{\infty}m(\delta, \pmb{x}) = 1 - \sum\limits_{\delta=0}^{a}m(\delta, \pmb{x})
$$

Substituting into the action function:

$$
a_{* }(t,b,\pmb x) = \arg\max\limits_{0 \leq a \leq b} \left\{ \sum\limits_{\delta =0}^{a}m(\delta, \pmb{x})\Big(\theta(\pmb x)+V_{* }(t-1,b-a)\Big) + 1 - \sum\limits_{\delta=0}^{a}m(\delta, \pmb{x})\Big(V_{* }(t-1,b)\Big) \right\}
$$

And since this is an argmax function we can remove constants:

$$
\begin{aligned}
a_{* }(t,b,\pmb x) & = \arg\max\limits_{0 \leq a \leq b} \left\{ \sum\limits_{\delta =0}^{a}m(\delta, \pmb{x})\Big(\theta(\pmb x)+V_{* }(t-1,b-a)\Big) - \sum\limits_{\delta=0}^{a}m(\delta, \pmb{x})\Big(V_{* }(t-1,b)\Big) \right\} \\
& = \arg\max\limits_{0 \leq a \leq b} \left\{ \sum\limits_{\delta =0}^{a}m(\delta, \pmb{x})\Big(\theta(\pmb x)+V_{* }(t-1,b-a)-V_{* }(t-1,b)\Big) \right\} \\
\end{aligned}
$$

And that's basically all we need to find the optimal actions! **The most important part for understanding this intuitively is:**

$$
g(a)=\Big(\theta(\pmb x)+V_{* }(t-1,b-a)-V_{* }(t-1,b)\Big)
$$

We refer to it as $$g(a)$$ for convenience. Basically, this indicates if winning an auction is worth it at a certain price. The first term is the predicted reward, and the second term is the value of the rest of the campaign if a certain amount $$a$$ is subtracted from the budget. The third, negative term is the value of the rest of the campaign if that same amount $$a$$ is saved rather than spent. If $$g(a)$$ is negative for a certain $$a$$, then the agent definitely should not make that bid.

The agent still needs to decide on values for $$a$$ where $$g(a)$$ is positive. Higher values of $$g(a)$$ imply lower values of $$a$$. Lower values of $$a$$, in turn, results in lower winrate $$\sum\nolimits_{\delta =0}^{a}m(\delta, \pmb{x})$$. In other words, high $$g(a)$$ implies low winrate, and vice versa. Since the agent needs to maximize the product of these two terms, it needs to strike a balance between winrate($$\sum\nolimits_{\delta =0}^{a}m(\delta, \pmb{x})$$) and value($$g(a)$$). After all, an extremely rewarding bid price isn't useful if there's no chance of winning the auction at that bid price.

**ADD BACKUP DIAGRAM**

Here's the full algorithm (Cai et al. 2017):

![Alt Text](../assets/img/rlb/rlb_algo.png){:height="700px" width="700px"}

As you can see, it finds all the value functions for every possible state first, and then uses those value functions to arrive at an optimal bid price. Obviously, considering every possible state combination takes a lot of computing power, so this solution becomes pretty much infeasible with large campaign sizes, such as $$T>10000$$.

## Solving the Large Scale MDP

The main source of complexity for this Algorithm 1 is finding the value functions. The computational complexity of this task of $$O(TB)$$, where $$T$$ is total campaign volume and $$B$$ is starting budget. To remedy this, Cai et al. 2017 fits a model to a small subset of a campaign ($$(t,b)\in \{0,...,T_0\}\times\{0,...,B_0\}$$) in order to approximate value functions for the full campaign data ($$(t,b)\in \{0,...,T\}\times\{0,...,B\}$$). Doing this allows one to avoid having to evaluate each value function directly.

In fact, we don't even need to approximate $$V_{* }(t,b)$$ directly. In $$g(x)$$, we're not concerned about the actual values functions themselves; we only care about the differential $$V_{* }(t-1,b-a)-V_{* }(t-1,b)$$. Here, we introduce a new value differential function:

$$
D(t,b)=V_{* }(t,b+1)-V_{* }(t,b)
$$

Which represents the smallest possible incremental difference between two states (this would be like if a bot bidded 1 cent on an ad). This works because we can substitute our previous differential like so:

$$
V_{* }(t-1,b-a)-V_{* }(t-1,b)=-\sum\limits_{\delta ' =1}^{\delta}D(t-1, b-\delta ')
$$

Think of our value differential $$V_{* }(t-1,b-a)-V_{* }(t-1,b)$$ as the negative sum of all the individual minimum increment differences. Therefore, all we need to do to estimate our value differential is to simply find how big the smallest increment $$D(t,b)$$ is for each bid price. To reiterate, $$D(t,b)$$ is our prediction target.

Cai et al. 2017 tried 3 methods to predict $$D(t,b)$$.

**Method 1: Straight neural nets (RLB-NN)**  
One can treat predicting $$D(t,b)$$ as a supervised learning task like any other. Cai et al. 2017 proves that $$D(t,b)$$ is nonlinear with respect to both $$t$$ and $$b$$, so Cai et al. used a fully-connected neural network with several hidden layers to model it directly. The two input nodes are $$b$$ and $$t$$ values, and the output node is $$D(t,b)$$. The neural net is trained on known values of $$D(t,b)$$ from the smaller campaign subset $$(t,b)\in \{0,...,T_0\}\times\{0,...,B_0\}$$. The result is denoted $$NN(t,b)$$, since the neural net is just a nonlinear function. $$NN(t,b)$$ is used as-is to construct value functions for the rest of the campaign data. Honestly, any non-linear supervised learning model would probably work fine as well.

The particular architecture used is a fully connected NN with 2 hidden layers, both of which have tanh activations. This first hidden layer has 30 nodes, and the second has 15 nodes. Dynamic programming is used to find the differentials $$D(t,b)$$ directly for a small campaign of $$T_0=10000$$. Then, the NN is trained on these known differentials, and then used to predict future differentials. 

**Method 2: Neural Nets + Coarse-to-fine Episode Segmentation (RLB-NN-Seg)**  
An alternative to straight supervised learning is to split the problem up into a series of smaller problems, and deal with them in sequence. This would work by dividing the full campaign of length $$T$$ into smaller campaigns of length $$T_0$$. Each sub-campaign gets a budget of $$B_r/N_r$$, where $$B_r$$ is the total remaining budget and $$N_r$$ is the remaining number of sub-campaigns.

For instance, the first sub-campaign would have a budget of $$B/N$$, where $$B$$ is the total budget and $$N$$ is the total number of sub-campaigns. The agent would either spend all it's sub-budget for sub-campaign 1, or it would have money remaining at the end of that sub-campaign. If there's money left over, then that remaining cash gets distributed evenly between the subsequent sub-campaigns.

We would choose these sub-campaigns to be of large enough size so that it's still impractical to calculate values directly, since making them too small would make it impossible for the agent to have enough time to adapt to each segment. Therefore, we would apply method 1 to each sub-campaign independently. In short, the only difference between this and method 1 is that method 2 splits the campaign into smaller episodes.

**Method 3: Neural Nets + Differential Mapping (RLB-NN-MapD)**  
Method 1 has a problem in that high values of $$t$$ will never be seen in the training data. After all, it's extremely expensive to calculate $$D(t,b)$$ explicitly for large campaigns. However, the NN will still have to predict $$D(t,b)$$ for high values of $$t$$, so we'll have to assume that its ability to generalize is good. However, instead of making such a naive solution, we can map large values of $$t$$ to smaller values that the NN has seen before, which should improve prediction accuracy.

Remember that the differential $$D(t,b)$$ refers to the change in value if the budget increases by 1. From this definition, we can deduce that $$D(t,b)$$ decreases w.r.t $$b$$, since having a large budget would mean that a single dollar difference doesn't matter as much. Similarly, $$D(t,b)$$ would increase w.r.t. $$t$$, since having a larger campaign means spreading out the same budget more thinly, which increases the value of each dollar. Therefore, these two variables don't affect value independently, and increasing $$t$$ is effectively decreasing $$b$$. From this, we can deduce that what *really* matters for $$D(t,b)$$ arent the values of $$t$$ and $$b$$ themselves, though they do affect the differential a bit. Instead, it's the ratio $$\frac{b}{t}$$ that's important. We'll call this ratio the **"budget condition"**. 

The differential mapping method itself starts by finding the all the $$D(t,b)$$ values for a sub-campaign of length $$T_0$$ and budget $$B_0$$. We want to keep this sub-campaign as large as possible for accuracy reasons, while maintaining reasonable computation costs. Therefore, instead of calculating $$D(t,b)$$ explicitly for the sub-campaign we'll follow method 1 and use neural nets to approximate differentials. This yields us $$NN(t,b)$$ for $$t<T_0$$. Then, to extrapolate to the rest of the campaign of length $$T$$, we use the neural net to predict $$D(T_0, \frac{b}{t}\times T_0)$$ for the rest of the $$t$$ values. Note that the NN doesn't have to make predictions for such high values of $$t>T_0$$ anymore; instead, it predicts on $$T_0$$, an input value that it's seen in the training set. This should boost accuracy greatly.

**Method 4: Neural Nets + Action Mapping (RLB-NN-MapA)**  
This final method is pretty much the same as method 3, but now just mapping actions directly instead of mapping $$D(t,b)$$. Again, we first take a sub-campaign and use a neural net to predict $$D(t,b)$$ where $$t<T_0$$. We then use this to get $$a(t,b,\pmb x)$$ for the sub-campaign. To get $$a(t,b,\pmb x)$$ values for the rest of the campaign $$T_0< t<T$$, we can use the following mapping:

$$
a(t,b,\pmb x) \rightarrow a(T_0, \frac{b}{t} \times T_0, \pmb x)
$$

The intuitive logic here is pretty much the same as in method 3; if a future budget condition is similar to the current one, then the optimal action should also be similar.

## RTB Solution Recap

To summarize, if a campaign is short enough then it can be solved using dynamic programming (**RLB**). However, in general, campaigns are far too big for dynamic programming to solve in a reasonable time. Therefore, for larger campaigns, we'll need to approximate some of the variables so that dynamic programming doesn't have to do all the work. There are four ways to do this:

1. **RLB-NN:** Use neural nets to output an approximation of $$D(t,b)$$ given inputs $$t$$ and $$b$$. Output is denoted $$NN(t,b)$$. NN trained on known values of $$D(t,b)$$ which are found using dynamic programming for a small portion of the campaign. 
2. **RLB-NN-Seg:** Divide a large campaign of length $$T$$ and budget $$B$$ into smaller sub-campaigns each with length $$T_0$$. Then, repeat **RLB-NN** for each sub campaign in sequence, starting the first campaign with budget $$\frac{B}{N}$$ where $$N$$ is the number of sub-campaigns. Remaining campaigns get leftover budget distributed among them, if any.
3. **RLB-NN-MapD:** Get $$NN(t,b)$$ for sub-campaign of length $$T_0$$. Then, instead of predicting $$D(t,b)$$ for $$t>T_0$$, map $$D(t,b)$$ to $$D(T_0, \frac{b}{t})$$ and get $$NN(T_0, \frac{b}{t}\times T_0)$$ to substitute for $$NN(t, b)$$ where $$t>T_0$$ or $$b>B_0$$.
4. **RLB-NN-MapA:** Get $$NN(t,b)$$ for sub-campaign of length $$T_0$$, and use it to get $$a(t,b,\pmb x)$$ for $$t<T_0$$ and $$b<B_0$$. Then, use $$a(T_0, \frac{b}{t}\times T_0, \pmb x)$$ to substitute for $$a(T_0, \frac{b}{t}\times T_0, \pmb x)$$ where $$t<T_0$$ or $$b<B_0$$.

# Experiments and Results:

Testing the methods listed above is a non-trivial problem, so in this section I'll be going over how Cai et al. 2017 tested and compared their strategies with other existing alternatives. They've made all their code freely available, and you can find it [here](https://github.com/han-cai/rlb-dp){:target="_ blank"}. All the bidding strategies share the same utility estimator and market landscape forecaster, which are both trained using the same training set.

## Alternative Bidding Strategies

1. **SS-MDP:** This is essentially the same as the small scale dynamic-programming-only solution (**RLB**), except it ignores feature vector $$\pmb x$$, and therefore doesn't attempt to predict the potential reward(CTR) of auctions. This was originally developed to work on keyword-based search advertising, where there are no individual feature vectors. It serves as a naive baseline in this experiment.
2. **Mcpc:** This strategy works like this:

$$
a_{Mcpc}(t,b,\pmb x)=CPC\times \theta(\pmb x)
$$

The goal of this strategy is to basically emulate a cost-per-click purchasing model which used to be the dominant model until RTB. The advertiser would set their desired CPC, and bid on each ad the CPC amount multiplied by the probability of click. This strategy treats each auction as an independent event, rather than a sequence. It also doesn't take into account market conditions.
3. **Lin:** Short for linear bidding strategy, **Lin** assumes that the optimal bid price is some linear function of the predicted click proability. The formula is the following:

$$
a_{Lin}(t,b,\pmb x)=b_0\times \frac{\theta(\pmb x)}{\theta_{avg}}
$$

Where $$b_0$$ is some parameter that's fitted using past data. Training $$b_0$$ captures budget condition, which means that **Lin** takes the budget into account implicitly. Intuitively, this method would bid more on ads that have a higher predicted CTR than average, and vice versa. While not as simple as **Mcpc**, **Lin** is still very simple and is the most widely used method in industry. **This is our primary baseline for assessing the effectiveness of the newer RL-based bidding strategies.**

## Offline evaluation:

Cai et al. 2017 used two datasets to evaluate bidding strategies offline: the [iPinYou dataset](http://data.computational-advertising.org/){:target="_ blank"} and the YOYI dataset (authors didn't link this dataset though).

Methods will be evaluated on the basis of total clicks, given a fixed campaign length and budget. Each model will be evaluated on a past auction dataset. In this dataset, each row represents an auction request, and contains the feature vector $$\pmb x$$ for that auction as well as its winning price. The dataset also contains information on whether or not each ad was clicked or not. This click response is used in the experiment as the KPI which each strategy is trying to maximize. Other metrics are also examined, like winrate, average cost/ad (CPM), and effective cost per click (eCPC).

*aside*: I actually have a bit of a problem with using past click response, since we don't know which company won the bid. If a past auction wasn't won by our company but we DO win it in an experimental run, then the fact that it was clicked may be irrelevant since the ad itself would've been much different.

An experiment follows these steps:
1. A campaign length and budget is initialized
2. One bid request feature vector $$\pmb x$$ is sent to the bidding agent.
3. The bidding agent submits a bid price for that request
4. The agent's bid request is compared with the actual known market price of that ad. If the agent outbids the market price, then it wins the auction, which results in a decreased budget and possibly a click. If the agent bids below the market price, then essentially nothing changes except the remaining ad volume, which decreases by 1.
5. Repeat steps 2-4 for $$T$$ ads, $$T$$ being the campaign length. 

Two datasets are used: the 

This experiment is repeated multiple times for various experiment parameters. The parameters that vary each experiment are campaign length,starting budget, and data. The campaign length will determine the size of the dataset that's fed to each agent. The starting budgets are determined in terms of the sum of the market prices of each ad in the campaign. For example, a budget parameter of 1/2 will allow an agent to win half of the auctions in a campaign. Experiments will be run with various budget parameters randing from 1/2 to 1/32. Finally, we can also change the data that's fed to each agent. Each campaign needs to be a chronologically consecutive chunk of auctions from the dataset, but we can change the chunk used from experiment to experiment.

### Small-scale offline evaluation:

The first test, which is a sort of a litmus test, is to test how the **RLB** method works on a small scale. For these experiments, $$T$$ is set to values less than 1500 and the tests focus on assessing the viability of the dynamic-programming-only solution, also known as the **RLB** algorithm.

First, let's see how **RTB** does under various budget conditions:

![Alt Text](../assets/img/rlb/small_budgets.png){:height="300px" width="600px"}
**Figure 2: Small scale evaluation results for various budgets (Cai et al. 2017)**
{: style="text-align: center"}

The top left graph is the most important one, and it shows that **RLB** does offer a bit of improvement over **Lin**. **Mcpc** does alright when given larger budgets, but does terribly with small budgets since it doesn't adapt to differing budgets, whereas **RLB** and **Lin** do. **SS-MDP** does terribly across the board despite taking budget condition into account, since it ignores the individual feature vectors of each auction. In terms of CPM and eCPC, **RLB** and **Lin** are basically the same, and while winrate does differ between the two methods, it isn't really a relevant metric. 

For various campaign lengths, **RTB** also offers some improvement over **Lin**:

![Alt Text](../assets/img/rlb/small_lengths.png){:height="300px" width="600px"}
**Figure 3: Small scale evaluation results for various campaign lengths (Cai et al. 2017)**
{: style="text-align: center"}

However, there aren't any noticeable patterns based on values of $$T$$. One mystery of figure 3 is how they achieved ~1700 clicks with a campaign of length 200, but this is probably just a plotting error.

Finally, here are the overall results of **RLB** vs **Lin**:

![Alt Text](../assets/img/rlb/small_results.png){:height="300px" width="600px"}
**Table 6: Click improvement of RLB vs Lin for  T=1000 and various budgets (Cai et al. 2017)**
{: style="text-align: center"}

In table 6, each row is a campaign of 1000 ads and each column is a budget condition. RLB wins the vast majority of the time, and performs especially well with small budgets. By the way, the YOYI row is a separate campaign, not an aggregate. We do have to be wary though of the fact that these could just be specific campaigns that happened to do extremely well, as these numbers definitely don't match up with the performances shown in figure 2. In fact, figure 2 never had **RLB** beating **Lin** by more than 10%, but table 6 shows this happening most of the time. Therefore, I'd take table 6 with a grain of salt, and rely more on the results of figure 2, which are still quite good.

These tests show that the underlying idea of bidding with reinforcement learning does provide a consistent, if small, improvement on the current industry standard when used in small campaigns. Now we just need to check if the results scale up to larger campaigns.

### Large Scale Offline Evaluation:

This is where we test our 4 large scale methods: **RLB-NN**, **RLB-NN-Seg**, **RLB-NN-MapA**, and **RLB-NN-MapD**. 

Altering $$T$$ didn't seems to affect results much in the small scale test, so Cai et al. 2017 decided not to alter it during large scale testing which is a good call. Large scale tests will be done with campaign length $$T=100000$$, which is equivalent to a medium-sized 10-minute campaign in the real world. 

Here are the results of the online test:

![Alt Text](../assets/img/rlb/large_results.png){:height="300px" width="600px"}
**Figure 4: Large scale evaluation results for various budgets (Cai et al. 2017)**
{: style="text-align: center"}

Like the small scale example, our RLB methods thrive with smaller budgets. **RLB-NN** didn't seem to do too well, but **RLB-NN-Seg**, **RLB-NN-MapA**, and **RLB-NN-MapD** consistently beat **Lin**.

## Online Evaluation:

I'll gloss over the technical details of the online test, but basically what they did was implement both **Lin** and **RLB** in real time. I'm not sure why they decided to only evaluate their small scale algorithm online, so I might be firing an email to ask about this.

Anyways, the two methods are given budgets, which are allocated to episodes, which are series of ads. Each episode contains a fraction of the budget, and have a limit of $$T$$ ads per episode. Episodes end when the budget is spent, or if it reaches the maximum volume $$T$$. Think of these episodes as small campaigns. Bid requests are recieved live over a 4-day period, and sent randomly to either **RLB** or **Lin**. Here are the results over the 4-day testing period:

![Alt Text](../assets/img/rlb/online_summary.png){:height="300px" width="600px"}
**Figure 5: Summary of results for online test (Cai et al. 2017)**
{: style="text-align: center"}

RLB did remarkably well, achieving 40% gain over **Lin** in terms of total clicks. Again, this is a bit suspicious since none of the offline tests came even close to such a result.

![Alt Text](../assets/img/rlb/online_over_time.png){:height="300px" width="600px"}
**Figure 6: Summary of results for online test (Cai et al. 2017)**
{: style="text-align: center"}

Figure 6 shows that the gap between **RLB** and **Lin** increases steadily over time. Overall, these are pretty great, if suspicious results. 

# Conclusion and Final Thoughts

Cai et al. 2017 present a **very** cool and novel method to optimize real-time bidding for display ads. It outperforms all current SotA bidding strategies due to a few things:

1. It treats auctions as sequential events, rather than independent events. This is much more representative of how campaigns work in the real world.
2. It considers and tries to predict the market price for an ad. This helps it avoid highballing the market price and wasting money.
3. It considers the number of remaining ads in a campaign, which other algorithms don't do.

All in all, this paper was a really cool and inspirational application of reinforcement techniques in a way that delivers direct value to the economy (unlike, for example, becoming a master Go player). What's even more interesting is that this paper makes use of dynamic programming, which is a rather classic RL technique discovered in the 50's. I think most people would've argued that this is an applied control theory paper rather than an RL paper, had the authors not used deep learning to augment their dynamic programming solution. This is exceptional since few papers nowadays contain applications of classical RL; most RL papers focus on more modern methods. Speaking of which, the authors are looking into more modern RL algos like Q-learning and policy gradient methods to combine utility estimation, bid landscape forecasting, and bid optimization into a unified optimization framework. It would be really cool if they can pull this off (and one of them might have with a recent [paper](https://arxiv.org/abs/1802.09756){:target="_ blank"}).

This paper wasn't without its faults though, as some of the experiment results seemed to either contradict each other or be implausible. I'll probably be emailing some of the authors about these issues. The paper did get accepted to WSDM 2017, which had a really low acceptance rate of %16, so it might have been me that made an error.

Anyways, I greatly enjoyed writing this, since it helped me solidify my understanding of the paper a lot better. I know it was a slog, but if you actually read this long post until this point, then I'm eternally grateful to you. 

Cheers!

*As always, if you notice any mistakes, have any feedback, or would just like to discuss this topic, send me a message at gaojohn23@gmail.com!*



# References
[1] Han Cai, Kan Ren, Weinan Zhang, Kleanthis Malialis, Jun Wang, Yong Yu, and Defeng Guo. 2017. [Real-time bidding by reinforcement learning in display advertising. ](https://arxiv.org/abs/1701.02490){:target="_ blank"}In Proceedings of the Tenth ACM International Conference on Web Search and Data Mining. ACM, 661670.

[2]K.-c. Lee, B. Orten, A. Dasdan, and W. Li. [Estimating conversion rate in display advertising from past performance data.](http://wnzhang.net/share/rtb-papers/cvr-est.pdf){:target="_ blank"} In KDD, 2012.

[3] T. Graepel, J. Q. Candela, T. Borchert, and R. Herbrich. [Web-scale bayesian click-through rate prediction for sponsored search advertising in Microsofts bing search engine. ](http://quinonero.net/Publications/AdPredictorICML2010-final.pdf){:target="_ blank"} In ICML, 2010.  

[4] H. B. McMahan, G. Holt, D. Sculley, M. Young, D. Ebner, J. Grady, L. Nie, T. Phillips, E. Davydov, D. Golovin, et al. [Ad click prediction: a view from the trenches. ](http://www.eecs.tufts.edu/~dsculley/papers/ad-click-prediction.pdf){:target="_ blank"} In KDD, 2013.

[5] Y. Cui, R. Zhang, W. Li, and J. Mao. [Bid landscape forecasting in online ad exchange marketplace. ](http://wnzhang.net/share/rtb-papers/bid-lands.pdf){:target="_ blank"} In KDD, 2011.

[6] W. C.-H. Wu, M.-Y. Yeh, and M.-S. Chen. [Predicting winning price in real time bidding with censored data. ](https://dl.acm.org/citation.cfm?id=2783276){:target="_ blank"} In KDD, 2015.

[7] W. Zhang, T. Zhou, J. Wang, and J. Xu. [Bid-aware gradient descent for unbiased learning with censored data in display advertising. ](http://wnzhang.net/papers/unbias-kdd.pdf){:target="_ blank"} In KDD

[8] C. Perlich, B. Dalessandro, R. Hook, O. Stitelman, T. Raeder, and F. Provost. [Bid optimizing and inventory scoring in targeted online advertising. ](https://dl.acm.org/citation.cfm?id=2339655){:target="_ blank"} In KDD, 2012.

[9] W. Zhang and J. Wang. [Statistical arbitrage mining for display advertising. ](https://arxiv.org/abs/1506.03837){:target="_ blank"} In KDD, 2015.

[10] J. Jin, C. Song, H. Li, K. Gai, J. Wang, and W. Zhang. [Real-Time Bidding with Multi-Agent Reinforcement Learning in Display Advertising ](https://arxiv.org/abs/1802.09756){:target="_ blank"} arXiv preprint arXiv:1802.09756.
